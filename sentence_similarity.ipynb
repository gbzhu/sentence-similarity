{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2_sentences(s1,s2):\n",
    "    return pd.DataFrame([(s1,s2,float(0))], columns=[\"sent_1\", \"sent_2\", \"sim\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_STS_dataset(file_path):\n",
    "    sent_pairs = []\n",
    "    with tf.io.gfile.GFile(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            sent_pairs.append((ts[5], ts[6], float(ts[4])))\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "STS_dev = load_STS_dataset(os.path.join(os.getcwd(),'sentence-similarity/data/stsbenchmark/sts-dev.csv'))\n",
    "STS_test = load_STS_dataset(os.path.join(os.getcwd(),'sentence-similarity/data/stsbenchmark/sts-test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A man with a hard hat is dancing.</td>\n",
       "      <td>A man wearing a hard hat is dancing.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A young child is riding a horse.</td>\n",
       "      <td>A child is riding a horse.</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man is feeding a mouse to a snake.</td>\n",
       "      <td>The man is feeding a mouse to the snake.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A woman is playing the guitar.</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A woman is playing the flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 sent_1  \\\n",
       "0     A man with a hard hat is dancing.   \n",
       "1      A young child is riding a horse.   \n",
       "2  A man is feeding a mouse to a snake.   \n",
       "3        A woman is playing the guitar.   \n",
       "4         A woman is playing the flute.   \n",
       "\n",
       "                                     sent_2   sim  \n",
       "0      A man wearing a hard hat is dancing.  5.00  \n",
       "1                A child is riding a horse.  4.75  \n",
       "2  The man is feeding a mouse to the snake.  5.00  \n",
       "3                  A man is playing guitar.  2.40  \n",
       "4                 A man is playing a flute.  2.75  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STS_dev[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sent_1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                             sent_2  sim  \n",
       "0                      A girl is brushing her hair.  2.5  \n",
       "1  A group of boys are playing soccer on the beach.  3.6  \n",
       "2           A woman measures another woman's ankle.  5.0  \n",
       "3                      A man is slicing a cucumber.  4.2  \n",
       "4                      A man is playing a keyboard.  1.5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STS_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T06:27:57.849192Z",
     "start_time": "2020-12-01T06:27:57.268611Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from threading import Semaphore\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T06:28:09.208690Z",
     "start_time": "2020-12-01T06:28:01.740132Z"
    }
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load('data/word2vec/GoogleNews-vectors-gensim-normed.bin',mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T06:35:47.244120Z",
     "start_time": "2020-12-01T06:35:31.517272Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('model.pickle','wb') as f:\n",
    "    pickle.dump(model,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T08:32:54.910483Z",
     "start_time": "2020-11-25T08:32:54.882013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/I342202/venvs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/I342202/venvs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.syn0norm = model.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T09:11:37.314903Z",
     "start_time": "2020-11-25T09:11:36.878435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('despise', 0.6712517142295837),\n",
       " ('Hate', 0.6400400400161743),\n",
       " ('detest', 0.6179036498069763),\n",
       " ('hatred', 0.6156139969825745),\n",
       " ('hating', 0.6103581190109253),\n",
       " ('hates', 0.6091769933700562),\n",
       " ('HATE', 0.6020098328590393),\n",
       " ('dislike', 0.6013234853744507),\n",
       " ('love', 0.600395679473877),\n",
       " ('hated', 0.5922117233276367)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T07:53:29.592269Z",
     "start_time": "2020-11-25T07:53:29.587507Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_WORD2VEC = os.path.join(os.getcwd(),'data/GoogleNews-vectors-negative300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T07:54:20.644113Z",
     "start_time": "2020-11-25T07:53:34.230424Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T08:03:03.567176Z",
     "start_time": "2020-11-25T08:02:49.862161Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T08:07:08.989271Z",
     "start_time": "2020-11-25T08:06:58.085475Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec.save(\"data/word2vec/GoogleNews-vectors-gensim-normed.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "PATH_TO_FREQUENCIES_FILE = os.path.join(os.getcwd(),'sentence-similarity/data/frequencies.tsv')\n",
    "PATH_TO_DOC_FREQUENCIES_FILE = os.path.join(os.getcwd(),'sentence-similarity/data/doc_frequencies.tsv')\n",
    "\n",
    "def read_tsv(f):\n",
    "    frequencies = {}\n",
    "    with open(f) as tsv:\n",
    "        tsv_reader = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        for row in tsv_reader: \n",
    "            frequencies[row[0]] = int(row[1])\n",
    "        \n",
    "    return frequencies\n",
    "        \n",
    "frequencies = read_tsv(PATH_TO_FREQUENCIES_FILE)\n",
    "doc_frequencies = read_tsv(PATH_TO_DOC_FREQUENCIES_FILE)\n",
    "doc_frequencies[\"NUM_DOCS\"] = 1288431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Smooth Inverse Frequency\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): \n",
    "\n",
    "    if doc_freqs is not None:\n",
    "        N = doc_freqs[\"NUM_DOCS\"]\n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            sims.append(0)\n",
    "            continue\n",
    "        \n",
    "        tokfreqs1 = Counter(tokens1)\n",
    "        tokfreqs2 = Counter(tokens2)\n",
    "        \n",
    "        weights1 = [tokfreqs1[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs1] if doc_freqs else None\n",
    "        weights2 = [tokfreqs2[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs2] if doc_freqs else None\n",
    "                \n",
    "        embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)\n",
    "        embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "        sims.append(sim)\n",
    "\n",
    "    return sims\n",
    "\n",
    "def remove_first_principal_component(X):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    pc = svd.components_\n",
    "    XX = X - X.dot(pc.transpose()) * pc\n",
    "    return XX\n",
    "\n",
    "\n",
    "def run_sif_benchmark(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=0.001): \n",
    "    total_freq = sum(freqs.values())\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # SIF requires us to first collect all sentence embeddings and then perform \n",
    "    # common component analysis.\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "        \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1.tokens\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2.tokens\n",
    "        \n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        weights1 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens1]\n",
    "        weights2 = [a/(a+freqs.get(token,0)/total_freq) for token in tokens2]\n",
    "        \n",
    "        embedding1 = np.average([model[token] for token in tokens1], axis=0, weights=weights1)\n",
    "        embedding2 = np.average([model[token] for token in tokens2], axis=0, weights=weights2)\n",
    "        \n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "        \n",
    "    embeddings = remove_first_principal_component(np.array(embeddings))\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), \n",
    "                              embeddings[idx*2+1].reshape(1, -1))[0][0] \n",
    "            for idx in range(int(len(embeddings)/2))]\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "STOP = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        self.tokens_without_stop = [t for t in self.tokens if t not in STOP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = scipy.stats.pearsonr(sims, df['sim'])[0]\n",
    "        print(label, pearson_correlation)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        spearman_correlation = scipy.stats.spearmanr(sims, df['sim'])[0]\n",
    "        spearman_cors.append(spearman_correlation)\n",
    "        \n",
    "    return pearson_cors, spearman_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "benchmarks = [(\"SIF-W2V\", ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIF-W2V 0.767571761029641\n",
      "SIF-W2V 0.68976121890589\n"
     ]
    }
   ],
   "source": [
    "pearson_results, spearman_results = {}, {}\n",
    "pearson_results[\"STS-DEV\"], spearman_results[\"STS-DEV\"] = run_experiment(STS_dev, benchmarks)\n",
    "pearson_results[\"STS-TEST\"], spearman_results[\"STS-TEST\"] = run_experiment(STS_test, benchmarks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_sents = load_2_sentences(\"A man with a hard hat is dancing.\",\"A man wearing a hard hat is dancing.\")\n",
    "s1 = [Sentence(\"A woman is playing the guitar.\")]\n",
    "s2 = [Sentence(\"A man is playing the guitar.\")]\n",
    "\n",
    "a = run_sif_benchmark(sentences1=s1,sentences2=s2,freqs=frequencies,model=word2vec,use_stoplist=True)\n",
    "\n",
    "b = run_sif_benchmark(sentences1=s1,sentences2=s2,freqs=frequencies,model=word2vec,use_stoplist=False)\n",
    "\n",
    "c =run_avg_benchmark(sentences1=s1,sentences2=s2,model=word2vec,use_stoplist=True,doc_freqs=doc_frequencies)\n",
    "\n",
    "d =run_avg_benchmark(sentences1=s1,sentences2=s2,model=word2vec,use_stoplist=False,doc_freqs=doc_frequencies)\n",
    "\n",
    "e =run_avg_benchmark(sentences1=s1,sentences2=s2,model=word2vec,use_stoplist=True)\n",
    "\n",
    "f =run_avg_benchmark(sentences1=s1,sentences2=s2,model=word2vec,use_stoplist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0000000000000004] [-0.9999999999999996] [0.9541760896030935] [0.9550532493934184] [0.9501258] [0.9625326]\n"
     ]
    }
   ],
   "source": [
    "print(a,b,c,d,e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
